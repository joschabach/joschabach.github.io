---
title: What kind of AI should we build if we want to understand the mind?
---

When Marvin Minsky, John McCarthy, Claude Shannon, Alan Newell, Herbert Simon and Nate Rochester started the field of Artificial Intelligence in the 1950ies, they were driven by an important insight: the study of the mind needed a different methodology than the one in any existing academic discipline. By then, psychology had turned into the study of human behavior, and curiously excluded introspection, the structure of mental representation and the functional architecture of mental processes from its research. Neuroscience, which took off at the same time, was not helping much, because it focused entirely on biological hardware. Philosophers lacked the tools and theoretical paradigms to get a grip on the nature of minds. However, a new class of models had sprung up: information theory, a generalized understanding of the nature of information processing: computation, and cybernetic systems allowed to see minds in a new light. Minsky understood that we need to put psychology on a computationalist foundation. With respect to the mind, brains are not chemical or biological systems, but information processing substrates. By building computer systems that behave in the same way as minds, we can express our theories, and test them. Minds are systems that make sense of the world by processing sensory information, deriving predictive and integrative models, forming conceptual abstractions, establishing goals, acting on them, reflecting, and so on. They have very specific architectures (very vaguely sketched by the psychoanalysts, but mostly left aside in mainstream psychology), developmental trajectories (as described, for instance, by Piaget) and a complex inner life (which our psychologists leave mostly to novelists to explore). Minsky was aware of that: he intended to build computer models that capture the mind in all its complexity and depth.

Minsky's idea of understanding the Rich Mind was not shared by all people in the field. Many groups preferred looking for narrow domain models, or focused on individual principles like learning, inference, planning, or self-organization. This was partially reflected in the distinction between Strong AI (dedicated to building minds) and Narrow AI (applications, and mathematical principles). This distinction may be less important, because the goals of AI as a cognitive science and AI as a field of engineering are clearly distinct and do not interfere much with each other. The more important difference exists within Strong AI itself: do we acknowledge the Rich Mind, or do we look for a Narrow Mind?

Of course it makes sense to build and test large theories step by step. When Claude Shannon looked for a less provocative headline than "Artificial Intelligence" to explain why he was interested in modeling the performance of playing Chess, he chose "Automata Studies". As we know, he was successful, but "guess what we got: automata studies" (McCarthy, 1973). Building and testing theories of the Rich Mind is hard. Before we can build a full model of the mind, only domain specific or mechanism specific approaches will yield success. Minsky pushed hard against too narrow models, but in doing so, burned the bridges between his own body of work–the rich Society of Mind theory–and connectionism, cybernetics, and signal processing.

Any deep perspective of human psychology, higher level thought processes, conceptual operations will have to go beyond the study of
